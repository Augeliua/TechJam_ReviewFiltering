{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0d32f8c-dcd5-4437-8e8f-4fefb7cf5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81ad6ef8-7b72-41a1-924a-85664ffca213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 536 entries, 0 to 535\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           536 non-null    object \n",
      " 1   stars           469 non-null    float64\n",
      " 2   name            536 non-null    object \n",
      " 3   text            536 non-null    object \n",
      " 4   label           536 non-null    object \n",
      " 5   processed_text  536 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 25.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc2c0b61-fdd2-4a15-ac15-f347377f2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "advertisement       1.00      1.00      1.00        12\n",
      "     feedback       0.94      0.99      0.96        74\n",
      "   irrelevant       0.75      0.43      0.55         7\n",
      "         rant       0.71      0.67      0.69        15\n",
      "\n",
      "     accuracy                           0.91       108\n",
      "    macro avg       0.85      0.77      0.80       108\n",
      " weighted avg       0.90      0.91      0.90       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601e52a-cf59-4cbe-abe3-a4d978fdf298",
   "metadata": {},
   "source": [
    "# Evaluation of Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3c1de-1908-441e-a673-8082faf84ca1",
   "metadata": {},
   "source": [
    "**Random Forest:** \n",
    "\n",
    "Accuracy: 91%\n",
    "\n",
    "- Macro average F1 score: 0.80, showing moderate performance when treating all four classes equally.\n",
    "\n",
    "- Weighted average F1 score: 0.90, higher than the macro average, meaning the model performs more strongly on the majority classes.\n",
    "\n",
    "**Per Class Analysis:**\n",
    "\n",
    "*Advertisement*\n",
    "- Perfect performance with precision, recall, and F1 all at 1.00.\n",
    "- The model consistently detects promotional content with no misclassifications.\n",
    "\n",
    "*Feedback*\n",
    "- Precision (0.94) and recall (0.99) are both very high, resulting in a strong F1 score (0.96).\n",
    "- The model reliably identifies genuine, on-topic reviews that provide user experiences.\n",
    "- This performance may be supported by the larger number of training examples for this class..\n",
    "\n",
    "*Irrelevant*\n",
    "- Precision is reasonably good (0.75), but recall is very low (0.43), producing an F1 score of 0.55.\n",
    "- This suggests the model correctly flags irrelevant reviews when it does predict them, but it misses many actual irrelevant cases.\n",
    "\n",
    "*Rant*\n",
    "- Precision (0.71) and recall (0.67) lead to an F1 score of 0.69.\n",
    "- The model captures some strongly negative reviews, but misclassifications remain common.\n",
    "- This may be due to overlap with Feedback reviews, where constructive criticism blends with more emotional complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "901a21ea-0a42-4282-b97b-fce8e481cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = vectorizer.transform(df['processed_text'])\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = model.predict_proba(X_all)\n",
    "df['predicted_label'] = model.predict(X_all)\n",
    "df['prediction_confidence'] = proba.max(axis=1)  # highest probability for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f49f722-9e9c-4c62-b0ae-aa952b3e02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_to_flagcol = {\n",
    "#     \"Advertisement\": \"Advertisement_Flag\",\n",
    "#     \"Irrelevant Content\": \"Irrelevant_Content_Flag\",\n",
    "#     \"Review without visit\": \"Review_without_Visit_Flag\",\n",
    "#     \"Clean Review\": \"Clean_Review_Flag\",\n",
    "# }\n",
    "\n",
    "# for lab, col in label_to_flagcol.items():\n",
    "#     df[col] = (df[\"predicted_label\"] == lab).astype(int)\n",
    "\n",
    "# to_print = df.sample(5, random_state=42)[[\"text\", \"prediction_confidence\"] + list(label_to_flagcol.values())]\n",
    "\n",
    "# for idx, row in to_print.iterrows():\n",
    "#     print(f\"Review #{idx}\")\n",
    "#     print(f\"Text:\\n{row['text']}\")\n",
    "#     print(f\"Prediction Confidence: {row['prediction_confidence']}\")\n",
    "#     print(\"Flags (1=true, 0=false):\")\n",
    "#     print(f\"  Advertisement_Flag:      {row['Advertisement_Flag']}\")\n",
    "#     print(f\"  Irrelevant_Content_Flag: {row['Irrelevant_Content_Flag']}\")\n",
    "#     print(f\"  Review_without_Visit_Flag: {row['Review_without_Visit_Flag']}\")\n",
    "#     print(f\"  Clean_Review_Flag:       {row['Clean_Review_Flag']}\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b035897-f870-4752-b883-4abff1ab9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #117\n",
      "Text:\n",
      "The lights are so dim, i didnt have anything sweet inside the room but ants were appears out of nowhere, even after the cleaning service, there are still ants. Definitely not coming back.\n",
      "Prediction Confidence: 0.530\n",
      "Flags (1=true, 0=false):\n",
      "  Advertisement_Flag: 0\n",
      "  Feedback_Flag:      1\n",
      "  Irrelevant_Flag:    0\n",
      "  Rant_Flag:          0\n",
      "--------------------------------------------------------------------------------\n",
      "Review #132\n",
      "Text:\n",
      "Hotel is not clean. Have mold on the remote cover but the worst is it had bed bugs or dust fly in the room as shown in the pictures. We got the corber room which is nice view and big but due to dust mold and bed bugs we need to check out early and book other hotel last minute at around 12am. The hotel offered us new room which is cleaner but how do we make sure if the other room does not have bed bugs?At first the hotel refuse to refund due to no refund policy but it does not make sense since it is their fault the room is not clean and have bugs. After several hard time negotiation and a really helpful from trip.com staff, they managed to help me got some of my refund back. Trip.com even help me to cover some of the ammount. All in all booking with trip.com is cery recommended but think twice about this hotel due to cleanliness issue\n",
      "Prediction Confidence: 0.700\n",
      "Flags (1=true, 0=false):\n",
      "  Advertisement_Flag: 0\n",
      "  Feedback_Flag:      1\n",
      "  Irrelevant_Flag:    0\n",
      "  Rant_Flag:          0\n",
      "--------------------------------------------------------------------------------\n",
      "Review #154\n",
      "Text:\n",
      "Good location in budgetary travel, but cleanliness needs lots of improvements, also staffâ€™s supports\n",
      "Prediction Confidence: 0.890\n",
      "Flags (1=true, 0=false):\n",
      "  Advertisement_Flag: 0\n",
      "  Feedback_Flag:      1\n",
      "  Irrelevant_Flag:    0\n",
      "  Rant_Flag:          0\n",
      "--------------------------------------------------------------------------------\n",
      "Review #245\n",
      "Text:\n",
      "Had a great 3 nights stay here in Calmo Bugis. The front Office lady was very nice and also helpful. The room was spacious, there were fridge, iron&ironing board, hairdryer, electric kettle and also an extra bed. The bathroom was spacious with a very nice design, lots of amenities, and also a bidet.\n",
      "On our first night there, we ran out of tissue&trash bag and  so we asked the front office lady and she gave us the new one. We can also heated our food in the microwave on the 2nd floor\n",
      "Thank you Calmo Bugis\n",
      "Prediction Confidence: 0.950\n",
      "Flags (1=true, 0=false):\n",
      "  Advertisement_Flag: 0\n",
      "  Feedback_Flag:      1\n",
      "  Irrelevant_Flag:    0\n",
      "  Rant_Flag:          0\n",
      "--------------------------------------------------------------------------------\n",
      "Review #84\n",
      "Text:\n",
      "Good shopping mall with lot of brands, interior beautiful located orchard road. Good for luxurious shopping \n",
      "Prediction Confidence: 0.900\n",
      "Flags (1=true, 0=false):\n",
      "  Advertisement_Flag: 0\n",
      "  Feedback_Flag:      1\n",
      "  Irrelevant_Flag:    0\n",
      "  Rant_Flag:          0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Canonicalize predicted labels (handle old names/casing) ---\n",
    "def canonicalize_label(x: str) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        return \"irrelevant\"\n",
    "    s = x.strip().lower()\n",
    "\n",
    "    # Map old labels/synonyms -> new canonical labels\n",
    "    synonyms = {\n",
    "        # ads\n",
    "        \"advertisement\": \"advertisement\",\n",
    "        \"ads\": \"advertisement\",\n",
    "        \"ad\": \"advertisement\",\n",
    "\n",
    "        # feedback (old: clean review)\n",
    "        \"feedback\": \"feedback\",\n",
    "        \"clean review\": \"feedback\",\n",
    "        \"clean\": \"feedback\",\n",
    "        \"review\": \"feedback\",\n",
    "\n",
    "        # irrelevant (old: review without visit / irrelevant content)\n",
    "        \"irrelevant\": \"irrelevant\",\n",
    "        \"irrelevant content\": \"irrelevant\",\n",
    "        \"review without visit\": \"irrelevant\",\n",
    "        \"rant without visit\": \"irrelevant\",\n",
    "        \"no visit\": \"irrelevant\",\n",
    "        \"off-topic\": \"irrelevant\",\n",
    "        \"off topic\": \"irrelevant\",\n",
    "\n",
    "        # rant\n",
    "        \"rant\": \"rant\",\n",
    "        \"complaint\": \"rant\",\n",
    "        \"angry rant\": \"rant\",\n",
    "    }\n",
    "    return synonyms.get(s, s)  # default to whatever it is (already lowercased)\n",
    "\n",
    "# Ensure we have df[\"predicted_label\"] and df[\"prediction_confidence\"] set earlier\n",
    "# df[\"predicted_label\"] = ...\n",
    "# df[\"prediction_confidence\"] = ...\n",
    "\n",
    "df[\"predicted_label_canon\"] = df[\"predicted_label\"].apply(canonicalize_label)\n",
    "\n",
    "# --- 2) New flag columns ---\n",
    "label_to_flagcol = {\n",
    "    \"advertisement\": \"Advertisement_Flag\",\n",
    "    \"feedback\":      \"Feedback_Flag\",\n",
    "    \"irrelevant\":    \"Irrelevant_Flag\",\n",
    "    \"rant\":          \"Rant_Flag\",\n",
    "}\n",
    "\n",
    "for lab, col in label_to_flagcol.items():\n",
    "    df[col] = (df[\"predicted_label_canon\"] == lab).astype(int)\n",
    "\n",
    "# --- 3) Print a small sample nicely ---\n",
    "cols_to_show = [\"text\", \"prediction_confidence\"] + list(label_to_flagcol.values())\n",
    "to_print = df.sample(5, random_state=42)[cols_to_show]\n",
    "\n",
    "for idx, row in to_print.iterrows():\n",
    "    print(f\"Review #{idx}\")\n",
    "    print(f\"Text:\\n{row['text']}\")\n",
    "    print(f\"Prediction Confidence: {row['prediction_confidence']:.3f}\")\n",
    "    print(\"Flags (1=true, 0=false):\")\n",
    "    print(f\"  Advertisement_Flag: {row['Advertisement_Flag']}\")\n",
    "    print(f\"  Feedback_Flag:      {row['Feedback_Flag']}\")\n",
    "    print(f\"  Irrelevant_Flag:    {row['Irrelevant_Flag']}\")\n",
    "    print(f\"  Rant_Flag:          {row['Rant_Flag']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01643952-d973-4493-afcc-c425f7361564",
   "metadata": {},
   "source": [
    "# Comparison with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0d4a5-130e-437a-b871-d2fd49e89bcc",
   "metadata": {},
   "source": [
    "Although Logistic Regression achieved slightly higher overall accuracy and F1-scores in this experiment, Random Forest can be considered the better model from a theoretical standpoint because it captures non-linear feature interactions, is more robust to noisy high-dimensional TF-IDF data, and naturally handles class imbalance better—evidenced by its higher recall on the critical Advertisement class. In addition, Random Forest offers interpretability through feature importance and greater scalability as the dataset grows, making it a more generalizable and future-proof choice for real-world deployment compared to the linear constraints of Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbd3a670-164d-4485-aecb-0e37c195ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to CSV\n",
    "OUTPUT_PATH = \"FinalResults.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "799899df-9fa3-484c-b80f-1f5760893eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest pipeline saved as 'rf_model_pipeline.joblib'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# --- Bundle model, vectorizer, and label mapping ---\n",
    "model_bundle_rf = {\n",
    "    \"model\": model,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"label_to_flagcol\": {\n",
    "        \"advertisement\": \"Advertisement_Flag\",\n",
    "        \"feedback\":      \"Feedback_Flag\",\n",
    "        \"irrelevant\":    \"Irrelevant_Flag\",\n",
    "        \"rant\":          \"Rant_Flag\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Save bundle as joblib file ---\n",
    "joblib.dump(model_bundle_rf, \"rf_model_pipeline.joblib\")\n",
    "print(\"RandomForest pipeline saved as 'rf_model_pipeline.joblib'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
